{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallotly/news-eval-cookbook/blob/main/Info_Extraction_Scenario_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Journalism Benchmark Cookbook: Information Extraction**\n",
        "In this notebook, we demonstrate how to evaluate an information extraction task based on a scenario with **textual information extraction** from **unstructured** **pdf files**.\n",
        "\n",
        "This notebook can be copied and modified to fit other similar scenarios of evaluation. For other specific tasks in information extraction, see here(link under construction)."
      ],
      "metadata": {
        "id": "Q-5JGF1TqZFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Overview\n",
        "\n",
        "*   **What is the use scenario?** In this use case, we aim to test the *information extraction* abilities of generative AI using a scenario in [MuckRock's reporting on governmental agencies' AI usage](https://www.muckrock.com/news/archives/2024/nov/26/foia-annual-reports-crowdsourced-ai/). This story examines the use of AI to assist with fulfilling FOIA requests in government agencies by looking at [Chief FOIA Officer Reports](https://www.documentcloud.org/documents/?q=%2Bproject%3Achief-foia-officer-report-219382%20) from 2023 and 2024, specifically looking for the answer to the question: \"Does your agency currently use any technology to automate record processing? For example, does your agency use machine learning, predictive coding, technology assisted review or similar tools to conduct searches or make redactions?\"\n",
        "\n",
        "*   **What is the dataset?** The dataset can be accessed at: https://github.com/MuckRock/crowdsource-foia-ai-reports. MuckRock employeed volunteers to read through these reports and provide annotations on how agencies reported using AI for each report. [Below](#data), we provide a brief explanation of data fields.\n",
        "\n",
        "*   **How is the test case set up?**\n",
        "We employ generative AI to extract information such as, year, agency name, and freeform text response regarding AI usage in [PDF reports](https://www.documentcloud.org/documents/?q=%2Bproject%3Achief-foia-officer-report-219382%20) submitted by agencies and obtained by MuckRock. <br> We prompt systems with a link to the report pdf and a text prompt: *Given the FOIA Officer report, please extract information about the year of the report, responding agency, whether the agency used machine learning or AI. If the agency reports using AI or machine learning, include the text in the report describing the agency's use of AI. Please generate output following this example json format: {'year': (string), 'agency': (string), 'ai_use': (boolean), 'original_text': (string)}*\n",
        "*   **How are we assessing the performance of gen AI?** For information such as year and agency name, we calculate an accuracy source based on exact match to the ground truth (collected by MuckRock through crowdsourcing). For freeform text extraction, we calculate the percent overlap between the model's response and the ground truth. Since the ground truth answers are crowdsourced and provided by users of MuckRock, we also compare a few entries of ground truth that express uncertainty to the model's response for the same documents.\n"
      ],
      "metadata": {
        "id": "_ocdJzPHGHlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"data\"></a>\n",
        "# 1. Data\n",
        "\n",
        "The dataset contains the following relevant fields :\n",
        "\n",
        "  * **datum**: documentcloud link to the pdf file (*e.g. https://www.documentcloud.org/documents/25178374-doc-2024-chief-foia-officer-report2*)\n",
        "  * **id**: documentcloud file id (e.g. *25178374*)\n",
        "  * **Which year is this report from?**: volunteer's response to which year the report is from. [*2023/2024/unknown*]\n",
        "  * **Which government office is this report from?**: volunteer's response for the government agency that produced the report (e.g. *U.S. Department of Commerce*)\n",
        "  * **Did the government office report that they do use machine learning or AI?**: volunteer's response for whether the government agency report using AI or not [*yes/no/unknown*]\n",
        "  * **If the government office does use AI or machine learning, copy and paste the text from the report that describes the office's use of the technology.**: volunteer's chosen excerpt from the document describing AI usage by the agency. (e.g. *We began using Relativity for deduplication.*)\n",
        "\n",
        "In total, there are 120 rows (corresponding to 120 reports produced by government agencies, each reviewed by 1 volunteer) in the dataset. This dataset is open sourced and used by MuckRock for a series of stories. It is worth noting that the volunteers' annotations were not verified beyond face value by the story editor, and it contains ambiguous answers, which are excluded from the evaluation below."
      ],
      "metadata": {
        "id": "1oz9zq2uP3Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Data Code\n",
        "The following code loads the data from github. The datum field is used to scrape urls to pdf files for use as input later."
      ],
      "metadata": {
        "id": "eTR9HRA7D9oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/MuckRock/crowdsource-foia-ai-reports/refs/heads/main/data/manual/results.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skCFJnWJaWZ1",
        "outputId": "1b856f04-4134-4618-d3d4-f7fb2c142281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 84327  100 84327    0     0   351k      0 --:--:-- --:--:-- --:--:--  353k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -O https://raw.githubusercontent.com/MuckRock/crowdsource-foia-ai-reports/refs/heads/main/data/manual/hand_annotated_flags.csv"
      ],
      "metadata": {
        "id": "RJXw_Z_BaMmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# load dataset containing links to pdfs\n",
        "df = pd.read_csv('results.csv')\n",
        "\n",
        "tasks = []\n",
        "for index, row in df.iterrows():\n",
        "  task = {}\n",
        "\n",
        "  # add id\n",
        "  task['id'] = row['id']\n",
        "\n",
        "  # get pdf url\n",
        "  url = row['datum']\n",
        "  html = requests.get(url)\n",
        "  soup = BeautifulSoup(html.text, 'html.parser')\n",
        "  for link in soup.find_all('a'):\n",
        "    if link.get('href').endswith(\"pdf\"):\n",
        "      task[\"url\"] = link.get('href')\n",
        "\n",
        "  # create groundtruth\n",
        "  groundtruth = {\n",
        "      'year': row['Which year is this report from?'],\n",
        "      'agency': row['Which government office is this report from?'],\n",
        "      'ai_use': row['Did the government office report that they do use machine learning or AI?'],\n",
        "      'original_text': row[\"If the government office does use AI or machine learning, copy and paste the text from the report that describes the office's use of the technology.\"]\n",
        "  }\n",
        "  task['groudtruth'] = groundtruth\n",
        "\n",
        "  # append to list\n",
        "  tasks.append(task)\n",
        "\n",
        "print(tasks[0])"
      ],
      "metadata": {
        "id": "oTSz6T5tMnj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4bd74f-92ce-404c-ce96-3f9d8e6036a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 25178374, 'url': 'https://s3.documentcloud.org/documents/25178374/doc-2024-chief-foia-officer-report2.pdf', 'groudtruth': {'year': '2024', 'agency': 'U.S. Department of Commerce', 'ai_use': 'Yes', 'original_text': 'The Departmentâ€™s technological priorities for the FOIA program over Fiscal Year 2024 include exploring possible artificial intelligence applications towards more efficient and timely processing while keeping transparency at the forefront and eDiscovery solutions for the FOIA program.\\n\\n Also, some components began using the EDR module within FOIAXpress to assist with responsive reviews of record sets found responsive to requests.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5qqS7IZKOzsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Task Details\n",
        "The scenario above outlines an *information extraction task*. This task is for the AI model to correctly extract specific information or excerpts from unstructured PDF documents and classify the information.\n",
        "\n",
        "This scenario fills a specific journalistic task context in that it is an *internal task*, which means the output of this task is used by internal newsroom staff, and it contains *unstructured sources* as opposed to structured data such as row-column format in a .CSV. This scenario focuses on data that is textual and that is stored in PDF files.\n",
        "\n",
        "We summarize the task context here:\n",
        "\n",
        "| Task Context | Value |\n",
        "| :----------- | :---- |\n",
        "| Task Usage   | Internal Usage (not public facing) |\n",
        "| File Format  | PDF |\n",
        "| Modality     | Text |\n",
        "| Data Structure | Unstructured |"
      ],
      "metadata": {
        "id": "ZjNjfRZnQBp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Task Code   \n",
        "The following code blocks first define a textual prompt input for models and then uses the [OpenRouter API](https://openrouter.ai/docs/quickstart#using-the-openrouter-api-directly) to collect model responses.\n",
        "\n",
        "A prompt is input with each document for 5 different models (`openai/gpt-5-mini`, `openai/gpt-oss-20b:free`, `anthropic/claude-opus-4.1`, `deepseek/deepseek-r1-0528:free`, `meta-llama/llama-3.3-70b-instruct`). The code uses OpenRouter's [file parser](https://openrouter.ai/docs/features/multimodal/pdfs) to input the pdf file in each prompt, and provides a template for [structured output](https://openrouter.ai/docs/features/structured-outputs).\n",
        "\n",
        "To authenticate the Open Router API with your own API key, refer to [this](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb#scrollTo=yeadDkMiISin) to create a separate name-value pair for Open Router. The total cost of running 120 prompts on 5 models came out to around $20.\n",
        "\n",
        "The last two code blocks loads model outputs as python `dict` for access in the next portion and exports outputs as a json file."
      ],
      "metadata": {
        "id": "4Hu5las9E_ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Given the FOIA Officer report, please extract information about the year of the report, responding agency, whether the agency used machine learning or AI. If the agency reports using AI or machine learning, include the text in the report describing the agency's use of AI. Please generate output following this example json format: {'year': (string), 'agency': (string), 'ai_use': (boolean), 'original_text': (string)}\""
      ],
      "metadata": {
        "id": "T-FDMmN4ZL5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('outputs_8:20.json','r') as f:\n",
        "  tasks = json.load(f)"
      ],
      "metadata": {
        "id": "JDEz5-nBeDAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"openai/gpt-5\",\"openai/gpt-5-mini\", \"openai/gpt-oss-20b:free\", \"anthropic/claude-opus-4.1\", \"deepseek/deepseek-r1-0528:free\", \"meta-llama/llama-3.3-70b-instruct\"]"
      ],
      "metadata": {
        "id": "JVCPxoEJdGdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import requests\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "url=\"https://openrouter.ai/api/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {userdata.get('openrouter_api')}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "plugins = [\n",
        "  {\n",
        "    \"id\": \"file-parser\",\n",
        "    \"pdf\": {\n",
        "        \"engine\": \"pdf-text\" # there is an option to natively input file, but not all models we are testing have file processing capabilities\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "response_format ={\n",
        "    \"type\": \"json_schema\",\n",
        "    \"json_schema\": {\n",
        "      \"name\": \"report_annotate\",\n",
        "      \"strict\": True,\n",
        "      \"schema\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"year\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"year of the report\"\n",
        "          },\n",
        "          \"agency\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"name of the agency\"\n",
        "          },\n",
        "          \"ai_use\": {\n",
        "            \"type\": \"boolean\",\n",
        "            \"description\": \"whether the report indicate use of AI\"\n",
        "          },\n",
        "          \"original_text\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"excerpt from the report indicating use of AI\"\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"year\", \"agency\", \"ai_use\", \"original_text\"],\n",
        "        \"additionalProperties\": False\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "for model in models:\n",
        "  print(f\"currently prompting {model}\")\n",
        "  for task in tqdm.tqdm(tasks[87:]):\n",
        "    #print(f\"task #{tasks.index(task)}\")\n",
        "    messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt,\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"file\",\n",
        "                    \"file\": {\n",
        "                      \"filename\": \"document.pdf\",\n",
        "                      \"file_data\": task['url']\n",
        "                      },\n",
        "                },\n",
        "              ],\n",
        "        }\n",
        "    ]\n",
        "    payload = {\n",
        "      \"model\": model,\n",
        "      \"messages\": messages,\n",
        "      \"response_format\": response_format,\n",
        "      \"plugins\": plugins\n",
        "    }\n",
        "    try:\n",
        "      response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "      raw_response = response.json()\n",
        "      task[model+'_output'] = raw_response['choices'][0]['message']['content']\n",
        "    except KeyError:\n",
        "      print(payload)\n",
        "      print(response.json())\n",
        "    except json.JSONDecodeError:\n",
        "      print(payload)\n",
        "      print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mke8dibHbEsh",
        "outputId": "cae5e8f9-e598-4f90-d74f-c14c282c05f4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currently prompting openai/gpt-5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33/33 [14:38<00:00, 26.61s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "for model in models:\n",
        "  for task in tqdm.tqdm(tasks):\n",
        "    if task[model+'_output'].startswith(\"{\") and task[model+'_output'].endswith(\"}\"):\n",
        "      task[model+\"_json\"] = json.loads(task[model+'_output'])\n",
        "    else:\n",
        "      try:\n",
        "        matches = re.findall(r\"\\{(.*?)\\}\", task[model+'_output'], re.S)\n",
        "        task[model+\"_json\"] = json.loads('{'+matches[0]+'}')\n",
        "      except IndexError:\n",
        "        print(\"index error\")\n",
        "        print(model)\n",
        "        print(tasks.index(task))\n",
        "        task[model+\"_json\"] = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AGjuofePzID",
        "outputId": "6952ff44-f9f5-4997-baee-513176c9b761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [00:00<00:00, 102341.70it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 51077.38it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 95560.37it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 40906.74it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 94466.31it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 76480.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index error\n",
            "meta-llama/llama-3.3-70b-instruct\n",
            "33\n",
            "index error\n",
            "meta-llama/llama-3.3-70b-instruct\n",
            "43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_oVS5EXeOj9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluation Metrics\n",
        "We focus on evaluating the *accuracy* of the models against annotations provided by volunteers on the FOIA report documents.\n",
        "\n",
        "*   For accuracy of \"year,\" \"government agency name,\" and \"whether the agency uses AI,\" we compare the generated answer against volunteer provided groundtruth based on exact match (i.e. whether the system generated answers that are exactly the same as the volunteers'). The value for these items range from 0 to 1 (0 being the system did not generated any answer that matches that of volunteers).\n",
        "*   For accuracy of the excerpt extracted from the original report that describes the use of AI, we compute the accumulated longest common substring length against answers provided by volunteers. To do this, we divide the length of a longest common substring between each generation and groundtruth by the length of ground truth, and we aggregate the average per system.\n",
        "\n",
        "#### Other Potential Metrics\n",
        "Other factors that could be measured includes speed, which would rely on measuring how fast the model produced a response, and uncertainty, which could either stem from directly prompting systems for a confidence score or prompting multiple times for a sample of outputs and computing a distribution. However, neither of these metrics are implemented here.\n",
        "\n"
      ],
      "metadata": {
        "id": "u_C-WaW2QSJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WbhA1yh9O_5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Performance Measurments"
      ],
      "metadata": {
        "id": "-gqqOXFDPBay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#    \n",
        "The following code blocks calculate the accuracy score for each models on each of the variables in the generated data (`year`, `ai_use`, `agency`, and `original_text`)."
      ],
      "metadata": {
        "id": "YCFRSzc1uisa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable"
      ],
      "metadata": {
        "id": "7o_KmS4mdZeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some ground truth entries do not include year, or include vague answers for the year\n",
        "valid_truth = 0\n",
        "for task in tasks:\n",
        "  if task['groudtruth']['year'] == \"2023\" or task['groudtruth']['year'] == \"2024\":\n",
        "    valid_truth += 1\n",
        "\n",
        "t = PrettyTable(['Model', 'Correct #', 'Accuracy'])\n",
        "\n",
        "# calculating accuracy with extracting the year of the report\n",
        "for model in models:\n",
        "  correct = 0\n",
        "  for task in tasks:\n",
        "    try:\n",
        "      correct += 1 if task['groudtruth']['year'] == task[model+'_json']['year'] else 0\n",
        "    except KeyError:\n",
        "      # print(tasks.index(task))\n",
        "      # print(model)\n",
        "      correct += 0\n",
        "  t.add_row([model, correct, correct/valid_truth])\n",
        "\n",
        "print(f\"The following table shows each model's accuracy score on {valid_truth} groundtruth answers for 'year'.\")\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhFgLJzDy7np",
        "outputId": "da9fc41c-4e53-4f4e-df6d-6f56bf4069bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following table shows each model's accuracy score on 102 groundtruth answers for 'year'.\n",
            "+-----------------------------------+-----------+--------------------+\n",
            "|               Model               | Correct # |      Accuracy      |\n",
            "+-----------------------------------+-----------+--------------------+\n",
            "|            openai/gpt-5           |     92    | 0.9019607843137255 |\n",
            "|         openai/gpt-5-mini         |     85    | 0.8333333333333334 |\n",
            "|      openai/gpt-oss-20b:free      |     87    | 0.8529411764705882 |\n",
            "|     anthropic/claude-opus-4.1     |     94    | 0.9215686274509803 |\n",
            "|   deepseek/deepseek-r1-0528:free  |     89    | 0.8725490196078431 |\n",
            "| meta-llama/llama-3.3-70b-instruct |     89    | 0.8725490196078431 |\n",
            "+-----------------------------------+-----------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating accuracy with identifying mention of ai uses\n",
        "\n",
        "t = PrettyTable(['Model', 'Correct #', 'Accuracy'])\n",
        "\n",
        "for model in models:\n",
        "  pos = 0\n",
        "  neg = 0\n",
        "  tp = 0\n",
        "  tn = 0\n",
        "  for task in tasks:\n",
        "    if task['groudtruth']['ai_use'] == 'Yes':\n",
        "      pos += 1\n",
        "      try:\n",
        "        if task[model+'_json']['ai_use'] == True:\n",
        "          tp += 1\n",
        "      except KeyError:\n",
        "        tp += 0\n",
        "    elif task['groudtruth']['ai_use'] == 'No':\n",
        "      neg += 1\n",
        "      try:\n",
        "        if task[model+'_json']['ai_use'] == False:\n",
        "          tn += 1\n",
        "      except KeyError:\n",
        "        tn += 0\n",
        "\n",
        "  t.add_row([model, tp+tn, (tp+tn)/(pos+neg)])\n",
        "\n",
        "print(f\"The following table shows each model's accuracy score on {pos+neg} groundtruth answers for 'ai use'.\")\n",
        "print(t)"
      ],
      "metadata": {
        "id": "3TtYSgoZ8EII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e643d01-c721-424a-ee0d-1fb679eab8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following table shows each model's accuracy score on 97 groundtruth answers for 'ai use'.\n",
            "+-----------------------------------+-----------+--------------------+\n",
            "|               Model               | Correct # |      Accuracy      |\n",
            "+-----------------------------------+-----------+--------------------+\n",
            "|            openai/gpt-5           |     67    | 0.6907216494845361 |\n",
            "|         openai/gpt-5-mini         |     69    | 0.711340206185567  |\n",
            "|      openai/gpt-oss-20b:free      |     69    | 0.711340206185567  |\n",
            "|     anthropic/claude-opus-4.1     |     67    | 0.6907216494845361 |\n",
            "|   deepseek/deepseek-r1-0528:free  |     72    | 0.7422680412371134 |\n",
            "| meta-llama/llama-3.3-70b-instruct |     71    | 0.7319587628865979 |\n",
            "+-----------------------------------+-----------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "valid_truth = 0\n",
        "for task in tasks:\n",
        "  if type(task['groudtruth']['agency']) == str:\n",
        "    valid_truth += 1\n",
        "\n",
        "t = PrettyTable(['Model', 'Correct #', 'Accuracy'])\n",
        "# calculating accuracy with extracting the agency name of the report\n",
        "for model in models:\n",
        "  matches = 0\n",
        "  for task in tasks:\n",
        "    if type(task['groudtruth']['agency']) == str:\n",
        "      try:\n",
        "        matches += 1 if task['groudtruth']['agency'].casefold() in task[model+'_json']['agency'].casefold() else 0\n",
        "      except KeyError:\n",
        "        matches += 0\n",
        "  t.add_row([model, matches, matches/valid_truth])\n",
        "\n",
        "print(f\"The following table shows each model's accuracy score on {valid_truth} groundtruth answers for 'agency name.'\")\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMMJmH7rgLZh",
        "outputId": "e3fb0408-3c10-4f19-bab8-f82af18b274a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following table shows each model's accuracy score on 109 groundtruth answers for 'agency name.'\n",
            "+-----------------------------------+-----------+--------------------+\n",
            "|               Model               | Correct # |      Accuracy      |\n",
            "+-----------------------------------+-----------+--------------------+\n",
            "|            openai/gpt-5           |     73    | 0.6697247706422018 |\n",
            "|         openai/gpt-5-mini         |     77    | 0.7064220183486238 |\n",
            "|      openai/gpt-oss-20b:free      |     72    | 0.6605504587155964 |\n",
            "|     anthropic/claude-opus-4.1     |     71    | 0.6513761467889908 |\n",
            "|   deepseek/deepseek-r1-0528:free  |     68    | 0.6238532110091743 |\n",
            "| meta-llama/llama-3.3-70b-instruct |     73    | 0.6697247706422018 |\n",
            "+-----------------------------------+-----------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "#code adapted from google\n",
        "def find_longest_common_substring(groundtruth, generated):\n",
        "    matcher = SequenceMatcher(None, groundtruth, generated)\n",
        "    overlap = matcher.find_longest_match(0, len(groundtruth), 0, len(generated))\n",
        "\n",
        "    if overlap.size == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return overlap.size/len(groundtruth)\n",
        "\n",
        "t = PrettyTable(['Model', 'Score'])\n",
        "for model in models:\n",
        "  score = 0\n",
        "  groundtruth = 0\n",
        "  for task in tasks:\n",
        "    try:\n",
        "      groundtruth_string = task['groudtruth']['original_text']\n",
        "      generated_string = task[model+'_json']['original_text']\n",
        "      if type(groundtruth_string) == str:\n",
        "        groundtruth += 1\n",
        "        if type(generated_string) == str:\n",
        "          score += find_longest_common_substring(groundtruth_string, generated_string)\n",
        "    except KeyError:\n",
        "      score += 0\n",
        "  t.add_row([model,score])\n",
        "\n",
        "print(f\"The following table shows each model's accuracy score on {groundtruth} groundtruth answers for 'original text.'\")\n",
        "print(t)"
      ],
      "metadata": {
        "id": "4JAEOv6Zbp2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0d4cd2-606a-464f-caa8-1a6ca08ac894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following table shows each model's accuracy score on 79 groundtruth answers for 'original text.'\n",
            "+-----------------------------------+--------------------+\n",
            "|               Model               |       Score        |\n",
            "+-----------------------------------+--------------------+\n",
            "|            openai/gpt-5           | 11.307042554095451 |\n",
            "|         openai/gpt-5-mini         | 17.177281269369416 |\n",
            "|      openai/gpt-oss-20b:free      | 10.471069697704849 |\n",
            "|     anthropic/claude-opus-4.1     | 17.79350369623537  |\n",
            "|   deepseek/deepseek-r1-0528:free  | 21.332857790887104 |\n",
            "| meta-llama/llama-3.3-70b-instruct | 35.146809715687674 |\n",
            "+-----------------------------------+--------------------+\n"
          ]
        }
      ]
    }
  ]
}